
cohere extension experiment to the paper : One Tokenizer to Rule Them All

training run for the universal tokenizer finetune : 

<img width="1796" height="857" alt="image" src="https://github.com/user-attachments/assets/97ceb2bf-4086-4fb8-843e-f349541f253b" />

------
<img width="1769" height="778" alt="image" src="https://github.com/user-attachments/assets/da28d4ab-c7e1-4c19-aa5b-35d9aef5303c" />


<img width="1753" height="772" alt="image" src="https://github.com/user-attachments/assets/d0ac2266-c4f0-4aca-ab82-cfcc4e53f3e7" />

<img width="1749" height="772" alt="image" src="https://github.com/user-attachments/assets/b17d569f-8c09-43d9-ac02-04b03b9da02d" />

-----
<img width="959" height="343" alt="image" src="https://github.com/user-attachments/assets/03b58485-a33a-4fd5-806e-4eb7780bf813" />

<img width="1774" height="784" alt="image" src="https://github.com/user-attachments/assets/91d73ee1-7e3c-4d66-bb31-0dab1d97e12f" />

<img width="1492" height="840" alt="image" src="https://github.com/user-attachments/assets/03b663d6-e2a4-4626-b84f-8c4106744106" />


<img width="1779" height="802" alt="image" src="https://github.com/user-attachments/assets/b4dfdf2b-f2df-4bf2-9e81-6b11c901d500" />
<img width="1067" height="403" alt="image" src="https://github.com/user-attachments/assets/a1254aff-cd73-44c6-ac3e-878379b85b0d" />

<img width="1770" height="780" alt="image" src="https://github.com/user-attachments/assets/1b844393-762e-4c4e-9fba-3fb2ff4b24a0" />




<img width="1781" height="781" alt="image" src="https://github.com/user-attachments/assets/32eb5764-5672-48e9-8f44-f4080a73df59" />

<img width="1784" height="789" alt="image" src="https://github.com/user-attachments/assets/76025291-d592-489b-9983-2709b0c94b8c" />

<img width="1226" height="690" alt="image" src="https://github.com/user-attachments/assets/673e91f4-af30-40a3-bed3-ec7667a3063c" />


